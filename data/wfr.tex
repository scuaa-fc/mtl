

\section{多任务系统中的网络结构设计问题}
多任务学习是一种联合多个任务同时学习来增强模型表示和泛化能力的一种手段\cite{liu2019end}，目前大都通过参数共享来实现多任务学习\cite{ma2019snr}。
对于多任务学习的深度神经网络结构设计部分，很多工作都集中在寻找更好的深度神经网络参数共享机制上\cite{ma2018modeling}，
而好的深度神经网络参数共享机制依赖于多任务学习任务集中不同任务之间的关系。正如本文第二章提到的，机场场面态势感知任务主要分为场景感知任务和场景检测任务两类，
本文在图\ref{fig_1}和图\ref{fig_2}中对这两类任务集中的任务关系进行了一个全面的梳理。
就多任务系统中的网络结构设计方面，目前已有的基于深度学习的多任务学习工作提出了很多参数共享的策略，其中使用的较多的有硬共享，软共享，分层共享，另外还有一些比较新颖的值得探索的共享机制，比如梯度共享，元共享等\cite{sun2020learning}\cite{ruder2017overview}\cite{zhang2018overview}\cite{kendall2018multi}\cite{kumar2021omnidet}。
本章从参数共享机制的角度对现有最常用的多任务深度神经网络模型进行了梳理，总结了现有的多任务深度神经网络结构设计中存在的关键问题，最后针对这些关键问题提出了一些优化的策略。

\subsection{多任务深度神经网络结构}
\subsubsection{基于参数硬共享的多任务深度神经网络结构}
参数硬共享是目前应用最为广泛的共享机制，它不同任务底层共享模型结构和参数，顶层分为几个不同的目标进行网络训练，这种结构本质上可以减少过拟合的风险，但是效果上可能受到任务差异和数据分布带来的影响。基本上，只要是能预测单模型的模型，都可以很简单的转化为参数硬共享模型的结构，只需要将共享层的最后一层与多个输出层拼接即可。
参数硬共享模型的原理如\ref{fig_1}所示。

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{fig/wfr/1_1.png}
		\caption{参数硬共享示意图}
		\label{fig_1}%label放在caption下面貌似才得行，要不然有问
	\end{center}
\end{figure}

接下来介绍两个典型的基于参数硬共享的多任务深度神经网络模型：

（1）MMoE多任务网络模型：
MMoE模型的结构基于广泛使用的Shared-Bottom结构和MoE结构\cite{lin2019pareto}\cite{mallya2018piggyback}，如图\ref{fig_2}所示，其中(a)是传统的硬共享参数模型，(b)是MoE模型，使用单个gate控制多个任务的参数,（c）是MMoE模型，在MoE的基础上，每个任务使用一个gate控制其权重。\ref{fig_2}中的expert指对模型输入进行不同方式的变换处理的网络层，每个Expert表示一种网络（Expert也可以都一样）。gate控制每个Expert权重的变量，对于每一个任务，不同Expert的权重可能是不一样的，因此使用gate来控制权重，类似于注意力机制\cite{maninis2019attentive}。

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{fig/wfr/1_2.png}
		\caption{MMoE网络结构示意图：a)共享浅层模型；b)单门MoE模型；c)多门MoE模型}
		\label{fig_2}%label放在caption下面貌似才得行，要不然有问
	\end{center}
\end{figure}

MoE模型对于不同的任务的gate权重是一样的，其函数表达式如公式\ref{eq_1}所示，其中k表示第k个任务，n表示n个expert网络。

\begin{equation}
\label{eq_1}
{y^k} = {h^k}\sum\limits_{i = 1}^n {{g_i}} {f_i}(x)
\end{equation}

MMoE是在MoE的基础上提出的方法，作者认为对于不同的任务，模型的权重选择是不同的，所以为每个任务分配一个gate模型。对于不同的任务，gate k的输出表示不同Expert被选择的概率，将多个Expert加权求和，得到$f_k(x)$，并输出给特点的Tower模型，用于最终的输出。 MMoE模型的表达式如公式\ref{eq_2}所示：

\begin{equation} \label{eq_2}
[{f^k}(x) = \sum\limits_{i = 1}^n {{g_i}^k(x)} {f_i}(x)]
\end{equation}

其中：

\begin{equation} \label{eq_3}
[{g^k}(x) = soft\max ({W_{{g^k}(x)}})]
\end{equation}

类似于MoE，k表示第k个任务，每个任务对应一个gate。MMoE的底层参数仍然是共享的，但是通过目标和网络参数直接的gate来学习，让每部分网络充分学习到对每个目标的贡献最大的一组参数结构，通过这种方式来保证底层网络参数共享的时候，不会出现目标之间相互抵消的作用。
MMoE模型从一定程度上解决了多个目标（任务）在训练过程中的相互耦合的问题，即使用门控概念降低了因为共享网络部分带来的特征耦合。但其实这是不够的，因为在每一个expert内部，与其他的expert不存在联系，这导致每个expert的表达能力不是很强\cite{misra2016cross}。

(2)SNR多任务网络模型：
当任务之间相关性比较强的时候，MTL可以学习到多个任务之间的关系，但是当任务之间相关性比较差的时候，在预估准确度上会比较差，主要是任务之间的干扰造成的共享网络部分难以收敛，MTL需要人工去调整模型的结构，以前的参数软共享在灵活性上和计算复杂度上不能兼得。同时，为了解决MMoE模型的局限性，SNR多任务学习模型被提出。该模型的作者提出了灵活参数共享的概念，即我们不应把共享网络部分作为整体的参数分享给每一个需要训练的目标，在共享网络的内部也需要互相共享参数，以提高表达\cite{rebuffi2018efficient}。通过设计了一款Sub-Network Routing模型将共享网络的上下层进行剥离，用下层中的所有参数作为上层输入共享。SNR多任务学习模型对任务之间的相关性强弱不敏感，借助简单的 NAS（Neural Architecture Search）\cite{ruder2019latent}\cite{strezoski2019many}，可以对自网络进行组合，学习一个好的模型结构。
其中，该模型的作者设计了两种共享方式：transformation和average，具体的网络结构如图\ref{fig_3}所示。

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{fig/wfr/1_3.png}
		\caption{SNR网络结构示意图：a)SB模型；b)SNR-Trans模型；c)SNR-Aver模型}
		\label{fig_3}%label放在caption下面貌似才得行，要不然有问
	\end{center}
\end{figure}

SNR多任务学习模型的主要优势是把网络结构的跨任务参数共享抽象为网络子结构的路由问题；引入0-1隐变量对路由作最优化；通过L1正则化（可以得到参数量更小的网络）,学到稀疏解，同等精度下节约11\%的网络结构开销\cite{strezoski2019learning}。

\subsubsection{基于参数软共享的多任务深度神经网络结构}
参数软共享不同于参数硬共享，每个任务有自己的参数，最后通过对不同任务的参数之间的差异加约束，表达相似性，比如可以使用L2正则化和迹范数等加以约束\cite{chennupati2019multinet++}\cite{dai2016instance}\cite{de2017guesswhat}。参数软共享模型的原理如图\ref{fig_4}所示。

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{fig/wfr/1_4.png}
		\caption{参数软共享示意图}
		\label{fig_4}%label放在caption下面貌似才得行，要不然有问
	\end{center}
\end{figure}

接下来介绍一个典型的基于参数软共享的多任务深度神经网络模型：MTAN多任务网络模型。MTAN结构主要包括两大部分，一个任务共享的主网络和K个特定任务的子网络，共享网络可以根据特定的任务进行设计，而每个特定于任务的子网络由一组注意力模块组成，这些模块与共享网络相连接。每个注意力模块对共享网络的特定层应用一个软注意力mask，以学习特定于任务的特征。基于这种设计，共享主网络可以看做是一个跨任务的特征表示，每一个注意力mask都可以被看作是对主网络的特征选择器，决定哪些共享特征被用到自己的子任务中去\cite{xu2018pad}。
\ref{fig_5}是MTAN的结构图，以VGG-16为主网络架构，因为SegNet的对称设计，只给出了编码器部分，任务一（绿色）和任务二（蓝色）的注意力模块，与共享网络（灰色）相连，决定了主网络那些特征会被利用到子网络中。中间是一个注意力模块的内部结构。

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{fig/wfr/1_5.png}
		\caption{MTAN网络结构示意图}
		\label{fig_5}%label放在caption下面貌似才得行，要不然有问
	\end{center}
\end{figure}

我们把注意力模块的内部结构再细化，如下图所示，该层的共享特（U）首先跟上一个注意力模块的输出进行Merge，具体方式为Concatenation(维度相加)，然后，将融合后的结果作为输入，经过下面这些操作：
1）g：1x1卷积，BN层，ReLu激活函数
2）h：1x1卷积，BN层，Sigmoid激活函数
3）a：得到对应的注意力mask
4）p：将注意力mask与主网络特征进行Element-wise Multiplication（对应元素相乘）
5）a：得到筛选出的特征
6）f：3x3卷积，BN层，ReLu激活函数

\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{fig/wfr/1_6.png}
		\caption{MTAN网络中的注意力模块示意图}
		\label{fig_6}%label放在caption下面貌似才得行，要不然有问
	\end{center}
\end{figure}

如图\ref{fig_6}所示，由于每个注意力模块中的mask是根据主网络相应层的共享特征学到的，因此两者可以联合学习，使得网络在不同任务中泛化性能更好，同时提升在特定任务中的表现。MTAN网络体系结构由一个全局特性池和特定任务的注意力模块组成，允许以端到端的方式自动学习任务共享和特定于任务的特性[33]。


\subsection{网络结构设计中的问题}
多任务网络模型的性能与不同任务的信息融合效率密切相关，基于参数硬共享的多任务深度神经网络模型把多个任务的数据表示嵌入到同一个语义空间中，再为每个任务使用单一任务特定层提取任务特定表示\cite{sener2018multi}\cite{lin2020controllable}。基于参数硬共享的多任务深度神经网络模型适合处理有较强相关性的任务，但遇到弱相关任务时常常表现很差\cite{smith2017federated}\cite{baltruvsaitis2018multimodal}\cite{ahn2019deep}。
而基于参数软共享的多任务深度神经网络模型为每个任务都学习一个网络，但每个任务的网络都可以访问其他任务对应网络中的信息，例如表示、梯度等。参数软共享机制非常灵活，不需要对任务相关性做任何假设，但是由于为每个任务分配一个网络，常常需要增加很多参数\cite{d2019sharing}\cite{doersch2017multi}，而且现有基于参数软共享的多任务深度神经网络模型中
不同任务的信息交互层面相对单一，通常只是对同一层网络信息进行交互，没有考虑不同任务间可能存在的尺度差异问题，例如机场场面态势感知中的检测任务和实例分割任务。

\subsection{优化策略}
针对上一节提到的多任务网络结构设计中面临的两个问题，本文提出了两种具有针对性的优化策略。其中，针对基于参数硬共享的多任务深度神经网络模型中弱相关任务表现差的问题，本文提出了一种基于任务路由机制的动态多任务深度神经网络结构进行优化；而针对基于参数软共享的多任务深度神经网络模型中不同任务信息交互层面单一问题，本文提出了一种多尺度信息蒸馏交互深度神经网络结构进行优化。

（1）基于任务路由机制的动态多任务深度神经网络结构：
通过在网络结构中引入任务路由层，它可以将任务特定的二进制掩码应用于共享网络部分的输出，归零化不适用于特定任务的特征，并有效地为各个任务分配一个与其他任务不同的子网络\cite{strezoski2019many}。在\cite{strezoski2019many}的基础上，将二进制0，1掩码修改为类似softmax操作的带权重特征融合掩码，同时实现对不适用特征的归零化和适用特征的有效融合。

（2）多尺度信息蒸馏交互深度神经网络结构：
在多任务学习设置中，提取任务信息时在多个尺度上考虑任务交互的重要性，在某个尺度上具有高亲和力的任务不能保证在其他尺度上保留这种行为，反之亦然。因此，可以考虑首先通过多尺度多模态蒸馏单元\cite{vandenhende2020mti}在每个尺度上显式地模拟任务交互。其次，通过特征传播模块将提取的任务信息从较低的尺度传播到较高的尺度。最后，通过一个特征聚合单元聚合来自所有尺度的细化任务特征，以产生最终的各个任务的预测结果。

\subsection{总结}
机场场面多任务态势感知问题主要分为场景感知任务和场景检测任务两类，其中场景感知任务通常为密集估计任务，而场景检测任务通常为非密集估计任务。
场景感知任务和场景检测任务间的的相关性通常较弱，因此采用基于任务路由机制的动态多任务神经网络结构可以有效地避免弱相关性任务在训练过程中的相互干扰问题。
且在机场场面多任务中，不同类型的任务之间信息传递的方式应不局限在同一个尺度上，因此，多尺度信息蒸馏交互神经网络结构有助于上述场景感知类任务和场景检测类任务的特征融合。

